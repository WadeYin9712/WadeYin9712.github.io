<html>
<head>
  <meta http-equiv="Content-Type" content="text/html; charset=utf-8" >
  <meta name="description" content="PhD student at University of California, Los Angeles (UCLA)">
  <title>Da Yin - UCLA</title>
  <link rel="shortcut icon" href="images/tree-12c-194370/tree-12c-152-194370.png" />
  <link href='assets/css_s/bootstrap.min.css' rel='stylesheet'>
  <link href='assets/css_s/main_style.css' rel='stylesheet'>

  <link href='https://fonts.googleapis.com/css?family=Nunito:200,300,400,500,600,700,800,900,100italic,100,300italic,400italic,500italic,900italic,700italic' rel='stylesheet' type='text/css'>
  <link href='https://fonts.googleapis.com/css?family=Lato:200,300,400,500,600,700,800,900,100italic,100,300italic,400italic,500italic,900italic,700italic' rel='stylesheet' type='text/css'>

  <!-- Global Site Tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-100825544-1"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-100825544-1');
  </script>
  <style type="text/css">
      .paper_rest{
        line-height: 1.5em
      }
      .row{
        line-height: 1.5em
      }
      .paper_title{
        font-size: 18px
      }
      .header-text-name{
        font-family:STFangsong
      }
  </style>
</head>
<body>
  <div id='header' class ='bg' style="font-family: Gill Sans">
    <div id='header-inner'>
      <div id="author-avatar"></div>
      <div class='header-text'>
        <div class='header-text-name'>
            Da Yin <span style="font-family:STFangsong; font-size:24pt"> </span>
        </div>
        <div class='header-text-email'>
          <p>PhD Student</p>
          <p>Computer Science Department, University of California, Los Angeles (UCLA)</p>
          <p>da.yin_@_cs.ucla.edu</p>
        </div>

        <div class='header-text-items'>
          <ul class="icons">
            <li><a href="files/da_yin_cv.pdf" class="icon fa-file-pdf-o"><span class="label"></span></a></li>
            <li><a href="https://github.com/WadeYin9712" class="icon fa-github"><span class="label"></span></a></li>
            <li><a href="https://twitter.com/Wade_Yin9712" class="icon fa-twitter"><span class="label"></span></a></li>
            <li><a href="https://scholar.google.com/citations?user=n32w34kAAAAJ&hl=en" class="icon ai-google-scholar"><span class="label"></span></a></li>
          </ul>
        </div>
      </div>
    </div>
  </div>

  <div class='container' style="font-size:18px; font-family: Gill Sans;">
    <div class='col-xs-1'>
    </div>
    <div class='col-xs-10'>
      <div class='row'>
        <p>
          I am a 4th-year PhD student in Computer Science at <a href="https://ucla.edu/">UCLA</a> advised by Prof. <a href="http://web.cs.ucla.edu/~kwchang/">Kai-Wei Chang</a>, working in <a href="http://web.cs.ucla.edu/~kwchang/members/">UCLANLP</a> lab. I was awarded <a href="https://www.sciencehub.ucla.edu/2022-amazon-fellows/">Amazon PhD Fellowship</a> in 2023. I previously interned at <a href="https://mosaic.allenai.org/">AI2 Mosaic Team</a> in Summer 2023, mentored by <a href="https://yuchenlin.xyz/">Bill Yuchen Lin</a> and <a href="https://homes.cs.washington.edu/~yejin/">Yejin Choi</a>; <a href="https://alexa.amazon.com/">Amazon Alexa AI</a> in Summer 2022, mentored by <a href="https://scholar.google.com/citations?user=amaLnocAAAAJ&hl=en">Feng Gao</a> and <a href="https://www.linkedin.com/in/govind-thattai-aaa5326/">Govind Thattai</a>.<br/> 
        </p>
        <p>
          I study Natural Language Processing (NLP) and intersection of NLP and Computer Vision (CV). My research interest is <b>building generalizable and inclusive language processing models</b> that can be applied across applications and regions. 
        </p>
        <p>
          I completed my bachelor's at <a href="https://www.pku.edu.cn/">Peking University</a>, where I worked with Prof. <a href="https://wanxiaojun.github.io/">Xiaojun Wan</a> and Prof. <a href="https://scholar.google.com.au/citations?user=LaKNyhQAAAAJ&hl=en">Baobao Chang</a>.
        </p>
        <!-- <p>
        You can find my <a href="files/CV.pdf">CV</a> here.
        </p> -->
        <!-- <b>NEWS</b>:
          <ul>
            <li>
            </li>
          </ul> -->
      </div>

      <div class='row' style="font-size:16px">
        <h2>Publications</h2>
        <table width="100%" align="center" border="0" cellspacing="0" cellpadding="0">
          <tr>
            <td valign="top">
              <div class='paper_title'>
                <a href="https://arxiv.org/abs/2407.19056">OfficeBench: Benchmarking Language Agents across Multiple Applications for Office Automation</a>
              </div>
              <div class='paper_rest'>
                Zilong Wang, Yuedong Cui, Li Zhong, Zimin Zhang, <b>Da Yin</b>, Bill Yuchen Lin, Jingbo Shang<br/>
                <i>Arxiv Preprint</i><br/>
                [ <a href="https://arxiv.org/abs/2407.19056">paper</a> | <a href="https://github.com/zlwang-cs/OfficeBench">code</a> ]
              </div>
              <div class="paper_bottom_space"></div>
            </td>
          </tr>
          <tr>
            <td valign="top">
              <div class='paper_title'>
                <a href="https://export.arxiv.org/abs/2407.14933">Consent in Crisis: The Rapid Decline of the AI Data Commons</a>
              </div>
              <div class='paper_rest'>
                Shayne Longpre, Robert Mahari, Ariel Lee, Campbell Lund, Hamidah Oderinwale, William Brannon, Nayan Saxena, Naana Obeng-Marnu, Tobin South, Cole Hunter, Kevin Klyman, Christopher Klamm, Hailey Schoelkopf, Nikhil Singh, Manuel Cherep, Ahmad Anis, An Dinh, Caroline Chitongo, <b>Da Yin</b>, Damien Sileo, Deividas Mataciunas, Diganta Misra, Emad Alghamdi, Enrico Shippole, Jianguo Zhang, Joanna Materzynska, Kun Qian, Kush Tiwary, Lester Miranda, Manan Dey, Minnie Liang, Mohammed Hamdy, Niklas Muennighoff, Seonghyeon Ye, Seungone Kim, Shrestha Mohanty, Vipul Gupta, Vivek Sharma, Vu Minh Chien, Xuhui Zhou, Yizhi Li, Caiming Xiong, Luis Villa, Stella Biderman, Hanlin Li, Daphne Ippolito, Sara Hooker, Jad Kabbara, Sandy Pentland<br/>
                <i>Arxiv Preprint</i><br/>
                <b><font color="#FF0000">Featured by <a href="https://www.nytimes.com/2024/07/19/technology/ai-data-restrictions.html">New York Times</a></font></b><br/>
                [ <a href="https://arxiv.org/abs/2407.14933">paper</a> | <a href="https://github.com/Data-Provenance-Initiative/Data-Provenance-Collection">code</a> | <a href="https://www.dataprovenance.org/">Project Page</a> ]
              </div>
              <div class="paper_bottom_space"></div>
            </td>
          </tr>
          <tr>
            <td valign="top">
              <div class='paper_title'>
                <a href="https://arxiv.org/abs/2311.05657">🪄 Agent Lumos: Unified and Modular Training for Open-Source Language Agents</a>
              </div>
              <div class='paper_rest'>
                <b>Da Yin</b>, Faeze Brahman, Abhilasha Ravichander, Khyathi Chandu, Kai-Wei Chang, Yejin Choi, Bill Yuchen Lin<br/>
                <b>ACL 2024, Presented at LLM Agent @ ICLR 2024</b><br/>
                <b><font color="#FF0000">Featured by <a href="https://www.marktechpost.com/2024/04/01/lumos-an-open-source-generalizable-language-agent-training-framework">Marktechpost</a></font></b><br/>
                [ <a href="https://arxiv.org/abs/2311.05657">paper</a> | <a href="https://github.com/allenai/lumos">code</a> <img alt="GitHub Repo stars" src="https://img.shields.io/github/stars/allenai/lumos?style=social"> | <a href="https://allenai.github.io/lumos/">Project Page</a> | <a href="https://huggingface.co/ai2lumos">Data & Models</a> | <a href="https://twitter.com/Wade_Yin9712/status/1724151646291169419">Twitter</a> ]
              </div>
              <div class="paper_bottom_space"></div>
            </td>
          </tr>
          <tr>
            <td valign="top">
              <div class='paper_title'>
                <a href="https://arxiv.org/abs/2403.02502">Trial and Error: Exploration-Based Trajectory Optimization for LLM Agents</a>
              </div>
              <div class='paper_rest'>
                Yifan Song, <b>Da Yin</b>, Xiang Yue, Jie Huang, Sujian Li, Bill Yuchen Lin<br/>
                <b>ACL 2024</b><br/>
                [ <a href="https://arxiv.org/abs/2403.02502">paper</a> | <a href="https://github.com/Yifan-Song793/ETO">code</a> | <a href="https://twitter.com/99Solaris/status/1765745995328200932">Twitter</a> ]
              </div>
              <div class="paper_bottom_space"></div>
            </td>
          </tr>
          <tr>
            <td valign="top">
              <div class='paper_title'>
                <a href="https://arxiv.org/abs/2303.15422">KPEval: Towards Fine-Grained Semantic-Based Keyphrase Evaluation</a>
              </div>
              <div class='paper_rest'>
                Di Wu, <b>Da Yin</b>, Kai-Wei Chang<br/>
                <b>ACL 2024 Findings</b><br/>
                [ <a href="https://arxiv.org/abs/2303.15422">paper</a> | <a href="https://github.com/uclanlp/KPEval">code</a> ]
              </div>
              <div class="paper_bottom_space"></div>
            </td>
          </tr>
          <tr>
            <td valign="top">
              <div class='paper_title'>
                <a href="https://arxiv.org/abs/2305.14327">Dynosaur: A Dynamic Growth Paradigm for Instruction-Tuning Data Curation</a>
              </div>
              <div class='paper_rest'>
                <b>Da Yin*</b>, Xiao Liu*, Fan Yin*, Ming Zhong*, Hritik Bansal, Jiawei Han, Kai-Wei Chang<br/>
                <i>(Equal Contribution)</i><br/>
                <b>EMNLP 2023</b><br/>
                [ <a href="https://arxiv.org/abs/2305.14327">paper</a> | <a href="https://github.com/WadeYin9712/Dynosaur">code</a> <img alt="GitHub Repo stars" src="https://img.shields.io/github/stars/WadeYin9712/Dynosaur?style=social"> | <a href="https://dynosaur-it.github.io/">Project Page</a> | <a href="https://twitter.com/Wade_Yin9712/status/1661237791920132098">Twitter</a> ]
              </div>
              <div class="paper_bottom_space"></div>
            </td>
          </tr>
          <tr>
            <td valign="top">
              <div class='paper_title'>
                <a href="">LEAF: Linguistically Enhanced Event Temporal Relation Framework</a>
              </div>
              <div class='paper_rest'>
                Stanley Lim, <b>Da Yin</b>, Nanyun Peng<br/>
                <b>Pan-DL workshop @ EMNLP 2023 (Pattern-based Approaches to NLP in the Age of Deep Learning)</b><br/>
                <b><font color="#FF0000">🏆 Best Paper Award</font></b><br/>
                [ <a href="https://aclanthology.org/2023.pandl-1.2/">paper</a> ]
              </div>
              <div class="paper_bottom_space"></div>
            </td>
          </tr>
          <tr>
            <td valign="top">
              <div class='paper_title'>
                <a href="">The Magic of IF: Investigating Causal Reasoning Abilities in Large Language Models of Code</a>
              </div>
              <div class='paper_rest'>
                Xiao Liu, <b>Da Yin</b>, Chen Zhang, Yansong Feng, Dongyan Zhao<br/>
                <b>ACL 2023 Findings</b><br/>
                [ <a href="https://arxiv.org/abs/2305.19213">paper</a> | <a href="https://github.com/xxxiaol/magic-if">code</a> ]
              </div>
              <div class="paper_bottom_space"></div>
            </td>
          </tr>
          <tr>
            <td valign="top">
              <div class='paper_title'>
                <a href="https://arxiv.org/abs/2301.01893">GIVL: Improving Geographical Inclusivity of Vision-Language Models with Pre-Training Methods</a>
              </div>
              <div class='paper_rest'>
                <b>Da Yin</b>, Feng Gao, Govind Thattai, Michael Johnston, Kai-Wei Chang<br/>
                <b>CVPR 2023</b><br/>
                [ <a href="https://arxiv.org/abs/2301.01893">paper</a> | code ]
              </div>
              <div class="paper_bottom_space"></div>
            </td>
          </tr>
          <tr>
            <td valign="top">
              <div class='paper_title'>
                <a href="https://arxiv.org/abs/2205.12247">GeoMLAMA: Geo-Diverse Commonsense Probing on Multilingual Pre-Trained Language Models</a>
              </div>
              <div class='paper_rest'>
                <b>Da Yin</b>, Hritik Bansal, Masoud Monajatipoor, Liunian Harold Li, Kai-Wei Chang<br/>
                <b>EMNLP 2022 <font color="#FF0000">(Oral, 175/4100≈4.3%)</font> </b><br/>
                [ <a href="https://arxiv.org/abs/2205.12247">paper</a> | <a href="https://github.com/WadeYin9712/GeoMLAMA/">code</a> ]
              </div>
              <div class="paper_bottom_space"></div>
            </td>
          </tr>
          <tr>
            <td valign="top">
              <div class='paper_title'>
                <a href="">How well can Text-to-Image Generative Models understand Ethical Natural Language Interventions?</a>
              </div>
              <div class='paper_rest'>
                Hritik Bansal*, <b>Da Yin*</b>, Masoud Monajatipoor, Kai-Wei Chang<br/>
                <i>(Equal Contribution)</i><br/>
                <b>EMNLP 2022 <font color="#FF0000">(Oral, 175/4100≈4.3%)</font> </b><br/>
                [ <a href="">paper</a> | <a href="">code</a> ]
              </div>
              <div class="paper_bottom_space"></div>
            </td>
          </tr>
          <tr>
            <td valign="top">
              <div class='paper_title'>
                <a href="">Towards a Unified Multi-Dimensional Evaluator for Text Generation</a>
              </div>
              <div class='paper_rest'>
                Ming Zhong, Yang Liu, <b>Da Yin</b>, Yuning Mao, Yizhu Jiao, Pengfei Liu, Chenguang Zhu, Heng Ji, Jiawei Han<br/>
                <b>EMNLP 2022 <font color="#FF0000">(Oral, 175/4100≈4.3%)</font> </b><br/>
                [ <a href="https://arxiv.org/abs/2210.07197">paper</a> | <a href="https://github.com/maszhongming/UniEval">code</a> <img alt="GitHub Repo stars" src="https://img.shields.io/github/stars/maszhongming/UniEval?style=social">]
              </div>
              <div class="paper_bottom_space"></div>
            </td>
          </tr>
          <tr>
            <td valign="top">
              <div class='paper_title'>
                <a href="https://arxiv.org/abs/2203.08075">Things not Written in Text: Exploring Spatial Commonsense from Visual Signals</a>
              </div>
              <div class='paper_rest'>
                Xiao Liu, <b>Da Yin</b>, Yansong Feng, Dongyan Zhao<br/>
                <b>ACL 2022</b><br/>
                [ <a href="https://arxiv.org/abs/2203.08075">paper</a> | <a href="https://github.com/xxxiaol/spatial-commonsense">code</a> ]
              </div>
              <div class="paper_bottom_space"></div>
            </td>
          </tr>
          <tr>
            <td valign="top">
              <div class='paper_title'>
                <a href="https://arxiv.org/abs/2202.08772">A Survey of Knowledge-Intensive NLP with Pre-Trained Language Models</a>
              </div>
              <div class='paper_rest'>
                <b>Da Yin</b>, Li Dong, Hao Cheng, Xiaodong Liu, Kai-Wei Chang, Furu Wei, Jianfeng Gao<br/>
                <i>Presented at Spa-NLP workshop @ ACL 2022 (Semiparametric Methods in NLP)</i><br/>
                [ <a href="https://arxiv.org/abs/2202.08772">paper</a> ]
              </div>
              <div class="paper_bottom_space"></div>
            </td>
          </tr>
          <tr>
            <td valign="top">
              <div class='paper_title'>
                <a href="https://aclanthology.org/2021.emnlp-main.162/">Broaden the Vision: Geo-Diverse Visual Commonsense Reasoning</a>
              </div>
              <div class='paper_rest'>
                <b>Da Yin</b>, Liunian Harold Li, Ziniu Hu, Nanyun Peng, Kai-Wei Chang<br/>
                <b>EMNLP 2021 <font color="#FF0000">(Oral, 315/3600≈8.8%)</font></b><br/>
                [ <a href="https://aclanthology.org/2021.emnlp-main.162/">paper</a> | <a href="https://github.com/WadeYin9712/GD-VCR">code</a> | <a href="https://gd-vcr.github.io/">Project Page</a> | <a href="files/EMNLP 2021 GD-VCR Slides.pdf">Slides</a> ]
              </div>
              <div class="paper_bottom_space"></div>
            </td>
          </tr>
          <tr>
            <td valign="top">
              <div class='paper_title'>
                <a href="https://arxiv.org/abs/2104.09420">Everything Has a Cause: Leveraging Causal Inference in Legal Text Analysis</a>
              </div>
              <div class='paper_rest'>
                Xiao Liu*, <b>Da Yin*</b>, Yansong Feng, Yuting Wu, Dongyan Zhao<br/>
                <i>(Equal Contribution)</i><br/>
                <b>NAACL 2021 <font color="#FF0000">(Oral)</font></b><br/>
                [ <a href="https://arxiv.org/abs/2104.09420">paper</a> | <a href="https://github.com/xxxiaol/GCI">code</a> ]
              </div>
              <div class="paper_bottom_space"></div>
            </td>
          </tr>
          <tr>
            <td valign="top">
              <div class='paper_title'>
                <a href="https://arxiv.org/abs/2104.05938">QMSum: A New Benchmark for Query-based Multi-domain Meeting Summarization</a>
              </div>
              <div class='paper_rest'>
                Ming Zhong*, <b>Da Yin*</b>, Tao Yu, Ahmad Zaidi, Mutethia Mutuma, Rahul Jha, Ahmed Hassan Awadallah, Asli Celikyilmaz, Yang Liu, Xipeng Qiu, Dragomir Radev<br/>
                <i>(Equal Contribution)</i><br/>
                <b>NAACL 2021 <font color="#FF0000">(Oral)</font></b><br/>
                [ <a href="https://arxiv.org/abs/2104.05938">paper</a> | <a href="https://github.com/Yale-LILY/QMSum">code</a> <img alt="GitHub Repo stars" src="https://img.shields.io/github/stars/Yale-LILY/QMSum?style=social">]
              </div>
              <div class="paper_bottom_space"></div>
            </td>
          </tr>
          <tr>
            <td valign="top">
              <div class='paper_title'>
                <a href="https://arxiv.org/abs/2005.04114">SentiBERT: A Transferable Transformer-Based Architecture for Compositional Sentiment Semantics</a>
              </div>
              <div class='paper_rest'>
                <b>Da Yin</b>, Tao Meng, Kai-Wei Chang<br/>
                <b>ACL 2020</b><br/>
                [ <a href="https://arxiv.org/abs/2005.04114">paper</a> | <a href="https://github.com/WadeYin9712/SentiBERT">code</a> ]
              </div>
              <div class="paper_bottom_space"></div>
            </td>
          </tr>
          <tr>
            <td valign="top">
              <div class='paper_title'>
                <a href="https://www.aclweb.org/anthology/2020.acl-main.469/">What Does BERT with Vision Look At?</a>
              </div>
              <div class='paper_rest'>
                Liunian Harold Li, Mark Yatskar, <b>Da Yin</b>, Cho-Jui Hsieh, Kai-Wei Chang<br/>
                <b>ACL 2020</b><br/>
                [ <a href="https://www.aclweb.org/anthology/2020.acl-main.469/">paper</a> | <a href="https://github.com/uclanlp/visualbert">code</a> ]
              </div>
              <div class="paper_bottom_space"></div>
            </td>
          </tr>
          <tr>
            <td valign="top">
              <div class='paper_title'>
                <a href="https://arxiv.org/abs/1908.03557">VisualBERT: A Simple and Performant Baseline for Vision and Language</a>
              </div>
              <div class='paper_rest'>
                Liunian Harold Li, Mark Yatskar, <b>Da Yin</b>, Cho-Jui Hsieh, Kai-Wei Chang<br/>
                <i>Arxiv Preprint</i><br/>
                [ <a href="https://arxiv.org/abs/1908.03557">paper</a> | <a href="https://github.com/uclanlp/visualbert">code</a> <img alt="GitHub Repo stars" src="https://img.shields.io/github/stars/uclanlp/visualbert?style=social">]
              </div>
              <div class="paper_bottom_space"></div>
            </td>
          </tr>
          <tr>
            <td valign="top">
              <div class='paper_title'>
                <a href="https://dl.acm.org/doi/10.1145/3357384.3358024">Interactive Multi-Grained Joint Model for Targeted Sentiment Analysis</a>
              </div>
              <div class='paper_rest'>
                <b>Da Yin*</b>, Xiao Liu*, Xiaojun Wan<br/>
                <i>(Equal Contribution)</i><br/>
                <b>CIKM 2019</b><br/>
                [ <a href="https://dl.acm.org/doi/10.1145/3357384.3358024">paper</a> ]
              </div>
              <div class="paper_bottom_space"></div>
            </td>
          </tr>
          <tr>
            <td valign="top">
              <div class='paper_title'>
                <a href="https://www.aclweb.org/anthology/W19-1302/">A Soft Label Strategy for Target-Level Sentiment Classification</a>
              </div>
              <div class='paper_rest'>
                <b>Da Yin</b>, Xiao Liu, Xiuyu Wu, Baobao Chang<br/>
                <b>WASSA workshop @ NAACL 2019 (Computational Approaches to Subjectivity, Sentiment and Social Media Analysis)</b><br/>
                [ <a href="https://www.aclweb.org/anthology/W19-1302/">paper</a> ]
              </div>
              <div class="paper_bottom_space"></div>
            </td>
          </tr>
        </table>
      </div>
      <br>
      <div class='row'>
        <hr>
      </div>

    </div>
    <div class='col-xs-1'>
    </div>
  </div>


</body>
</html>

<!-- Localized -->
